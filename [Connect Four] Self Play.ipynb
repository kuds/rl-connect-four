{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOpY1Smoe7ozBF801eokgbQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-connect-four/blob/main/%5BConnect%20Four%5D%20Self%20Play.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playing Connect Four using Self Play"
      ],
      "metadata": {
        "id": "9ddHPfUHmowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl"
      ],
      "metadata": {
        "id": "TpZLZdJK-SAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gputil open_spiel gymnasium"
      ],
      "metadata": {
        "id": "_mfyOqFelr3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.air.constants import TRAINING_ITERATION\n",
        "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
        "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
        "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
        "from ray.rllib.env.utils import try_import_pyspiel, try_import_open_spiel\n",
        "from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv\n",
        "from ray.rllib.examples.rl_modules.classes.random_rlm import RandomRLModule\n",
        "from ray.rllib.examples.multi_agent.utils import (\n",
        "    ask_user_for_action,\n",
        "    SelfPlayCallback,\n",
        "    SelfPlayCallbackOldAPIStack,\n",
        ")\n",
        "from ray.rllib.examples._old_api_stack.policy.random_policy import RandomPolicy\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "from ray.rllib.utils.metrics import NUM_ENV_STEPS_SAMPLED_LIFETIME\n",
        "from ray.rllib.utils.test_utils import (\n",
        "    add_rllib_example_script_args,\n",
        "    run_rllib_example_script_experiment,\n",
        ")\n",
        "from ray.tune.registry import get_trainable_cls, register_env\n",
        "import platform\n",
        "\n",
        "import torch\n",
        "from importlib.metadata import version\n",
        "\n",
        "# Import after try_import_open_spiel, so we can error out with hints.\n",
        "from open_spiel.python.rl_environment import Environment  # noqa: E402"
      ],
      "metadata": {
        "id": "lJUAoPZeiGag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('open_spiel')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")\n",
        "print(f\"Gymnasium Version: {version('Gymnasium')}\")\n",
        "print(f\"Open Spiel Version: {version('open_spiel')}\")"
      ],
      "metadata": {
        "id": "jvbn9aJGh80w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number or CPUs Available: {mp.cpu_count()}\")"
      ],
      "metadata": {
        "id": "hcLCpu7_RH0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_spiel = try_import_open_spiel(error=True)\n",
        "pyspiel = try_import_pyspiel(error=True)"
      ],
      "metadata": {
        "id": "HkHVtp4zi_fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.env = \"connect_four\"\n",
        "        self.checkpoint_freq = 1\n",
        "        self.checkpoint_at_end = True\n",
        "        self.win_rate_threshold = 0.95\n",
        "        self.min_league_size = 3\n",
        "        self.num_episodes_human_play = 10\n",
        "        self.from_checkpoint = None\n",
        "        # Add other necessary attributes from parser arguments\n",
        "        self.algo = 'PPO' # Assuming PPO is the default algorithm\n",
        "        self.num_env_runners = 2\n",
        "        self.enable_new_api_stack = True\n",
        "        self.stop_timesteps = 2000000\n",
        "        self.stop_iters = 100\n",
        "        self.as_release_test = False\n",
        "        self.num_cpus = 10\n",
        "        self.local_mode = False\n",
        "        self.framework = 'torch'\n",
        "        self.num_gpus = 0\n",
        "        self.num_gpus_per_learner = 1\n",
        "        self.num_learners = 1\n",
        "        self.evaluation_interval = 0\n",
        "        self.log_level = None\n",
        "        self.output = None\n",
        "        self.no_tune = False\n",
        "        self.num_agents = 0\n",
        "        self.verbose = 2\n",
        "        self.num_samples = 1\n",
        "        self.max_concurrent_trials = None\n",
        "        self.as_test = False\n",
        "        self.num_envs_per_env_runner = 1 if args.enable_new_api_stack else 5\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "n14z0JJF-eom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_to_module_mapping_fn(agent_id, episode, **kwargs):\n",
        "        # agent_id = [0|1] -> module depends on episode ID\n",
        "        # This way, we make sure that both modules sometimes play agent0\n",
        "        # (start player) and sometimes agent1 (player to move 2nd).\n",
        "        return \"main\" if hash(episode.id_) % 2 == agent_id else \"random\"\n",
        "\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "    return \"main\" if episode.episode_id % 2 == agent_id else \"random\"\n",
        "\n",
        "config = (\n",
        "    get_trainable_cls(args.algo)\n",
        "    .get_default_config()\n",
        "    .environment(\"open_spiel_env\")\n",
        "    # Set up the main piece in this experiment: The league-bases self-play\n",
        "    # callback, which controls adding new policies/Modules to the league and\n",
        "    # properly matching the different policies in the league with each other.\n",
        "    .callbacks(\n",
        "        functools.partial(\n",
        "            (\n",
        "                SelfPlayCallback\n",
        "                if args.enable_new_api_stack\n",
        "                else SelfPlayCallbackOldAPIStack\n",
        "            ),\n",
        "            win_rate_threshold=args.win_rate_threshold,\n",
        "        )\n",
        "    )\n",
        "    .env_runners(\n",
        "        num_env_runners=(args.num_env_runners or 2),\n",
        "        num_envs_per_env_runner=1 if args.enable_new_api_stack else 5,\n",
        "    )\n",
        "    .multi_agent(\n",
        "        # Initial policy map: Random and default algo one. This will be expanded\n",
        "        # to more policy snapshots taken from \"main\" against which \"main\"\n",
        "        # will then play (instead of \"random\"). This is done in the\n",
        "        # custom callback defined above (`SelfPlayCallback`).\n",
        "        policies=(\n",
        "            {\n",
        "                # Our main policy, we'd like to optimize.\n",
        "                \"main\": PolicySpec(),\n",
        "                # An initial random opponent to play against.\n",
        "                \"random\": PolicySpec(policy_class=RandomPolicy),\n",
        "            }\n",
        "            if not args.enable_new_api_stack\n",
        "            else {\"main\", \"random\"}\n",
        "        ),\n",
        "        # Assign agent 0 and 1 randomly to the \"main\" policy or\n",
        "        # to the opponent (\"random\" at first). Make sure (via episode_id)\n",
        "        # that \"main\" always plays against \"random\" (and not against\n",
        "        # another \"main\").\n",
        "        policy_mapping_fn=(\n",
        "            agent_to_module_mapping_fn\n",
        "            if args.enable_new_api_stack\n",
        "            else policy_mapping_fn\n",
        "        ),\n",
        "        # Always just train the \"main\" policy.\n",
        "        policies_to_train=[\"main\"],\n",
        "    )\n",
        "    .rl_module(\n",
        "        model_config=DefaultModelConfig(fcnet_hiddens=[512, 512]),\n",
        "        rl_module_spec=MultiRLModuleSpec(\n",
        "            rl_module_specs={\n",
        "                \"main\": RLModuleSpec(),\n",
        "                \"random\": RLModuleSpec(module_class=RandomRLModule),\n",
        "            }\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Only for PPO, change the `num_epochs` setting.\n",
        "if args.algo == \"PPO\":\n",
        "    config.training(num_epochs=20)\n",
        "\n",
        "stop = {\n",
        "    NUM_ENV_STEPS_SAMPLED_LIFETIME: args.stop_timesteps,\n",
        "    TRAINING_ITERATION: args.stop_iters,\n",
        "    \"league_size\": args.min_league_size,\n",
        "}\n",
        "\n",
        "# Train the \"main\" policy to play really well using self-play.\n",
        "results = None\n",
        "if not args.from_checkpoint:\n",
        "    results = run_rllib_example_script_experiment(\n",
        "        config, args, stop=stop\n",
        "    )\n",
        "\n",
        "# Restore trained Algorithm (set to non-explore behavior) and play against\n",
        "# human on command line.\n",
        "if args.num_episodes_human_play > 0:\n",
        "    num_episodes = 0\n",
        "    config.explore = False\n",
        "    algo = config.build()\n",
        "    if args.from_checkpoint:\n",
        "        algo.restore(args.from_checkpoint)\n",
        "    else:\n",
        "        checkpoint = results.get_best_result().checkpoint\n",
        "        if not checkpoint:\n",
        "            raise ValueError(\"No last checkpoint found in results!\")\n",
        "        algo.restore(checkpoint)\n",
        "\n",
        "    if args.enable_new_api_stack:\n",
        "        rl_module = algo.get_module(\"main\")\n",
        "\n",
        "    # Play from the command line against the trained agent\n",
        "    # in an actual (non-RLlib-wrapped) open-spiel env.\n",
        "    human_player = 1\n",
        "    env = Environment(args.env)\n",
        "\n",
        "    while num_episodes < args.num_episodes_human_play:\n",
        "        print(\"You play as {}\".format(\"o\" if human_player else \"x\"))\n",
        "        time_step = env.reset()\n",
        "        while not time_step.last():\n",
        "            player_id = time_step.observations[\"current_player\"]\n",
        "            if player_id == human_player:\n",
        "                action = ask_user_for_action(time_step)\n",
        "            else:\n",
        "                obs = np.array(time_step.observations[\"info_state\"][player_id])\n",
        "                if args.enable_new_api_stack:\n",
        "                    action = np.argmax(\n",
        "                        rl_module.forward_inference(\n",
        "                            {\"obs\": torch.from_numpy(obs).unsqueeze(0).float()}\n",
        "                        )[\"action_dist_inputs\"][0].numpy()\n",
        "                    )\n",
        "                else:\n",
        "                    action = algo.compute_single_action(obs, policy_id=\"main\")\n",
        "                # In case computer chooses an invalid action, pick a\n",
        "                # random one.\n",
        "                legal = time_step.observations[\"legal_actions\"][player_id]\n",
        "                if action not in legal:\n",
        "                    action = np.random.choice(legal)\n",
        "            time_step = env.step([action])\n",
        "            print(f\"\\n{env.get_state}\")\n",
        "\n",
        "        print(f\"\\n{env.get_state}\")\n",
        "\n",
        "        print(\"End of game!\")\n",
        "        if time_step.rewards[human_player] > 0:\n",
        "            print(\"You win\")\n",
        "        elif time_step.rewards[human_player] < 0:\n",
        "            print(\"You lose\")\n",
        "        else:\n",
        "            print(\"Draw\")\n",
        "        # Switch order of players.\n",
        "        human_player = 1 - human_player\n",
        "\n",
        "        num_episodes += 1\n",
        "\n",
        "    algo.stop()"
      ],
      "metadata": {
        "id": "A5y6PBpEmTCW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}