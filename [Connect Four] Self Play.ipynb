{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNuossC8vnO3Zqv1C8fq8DW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-connect-four/blob/main/%5BConnect%20Four%5D%20Self%20Play.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playing Connect Four using Self Play"
      ],
      "metadata": {
        "id": "9ddHPfUHmowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] gputil open_spiel"
      ],
      "metadata": {
        "id": "_mfyOqFelr3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "from ray.air.constants import TRAINING_ITERATION\n",
        "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
        "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
        "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
        "from ray.rllib.env.utils import try_import_pyspiel, try_import_open_spiel\n",
        "from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv\n",
        "from ray.rllib.examples.rl_modules.classes.random_rlm import RandomRLModule\n",
        "from ray.rllib.examples.multi_agent.utils import (\n",
        "    ask_user_for_action,\n",
        "    SelfPlayCallback,\n",
        "    SelfPlayCallbackOldAPIStack,\n",
        ")\n",
        "from ray.rllib.examples._old_api_stack.policy.random_policy import RandomPolicy\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "from ray.rllib.utils.metrics import NUM_ENV_STEPS_SAMPLED_LIFETIME\n",
        "from ray.rllib.utils.test_utils import (\n",
        "    add_rllib_example_script_args,\n",
        "    run_rllib_example_script_experiment,\n",
        ")\n",
        "from ray.tune.registry import get_trainable_cls, register_env\n",
        "import platform\n",
        "\n",
        "import torch\n",
        "from importlib.metadata import version\n",
        "\n",
        "# Import after try_import_open_spiel, so we can error out with hints.\n",
        "from open_spiel.python.rl_environment import Environment  # noqa: E402"
      ],
      "metadata": {
        "id": "lJUAoPZeiGag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('open_spiel')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")\n",
        "print(f\"Gymnasium Version: {version('Gymnasium')}\")\n",
        "print(f\"Open Spiel Version: {version('open_spiel')}\")"
      ],
      "metadata": {
        "id": "jvbn9aJGh80w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_spiel = try_import_open_spiel(error=True)\n",
        "pyspiel = try_import_pyspiel(error=True)"
      ],
      "metadata": {
        "id": "HkHVtp4zi_fQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "        self.env = \"connect_four\"\n",
        "        self.framework = \"torch\"\n",
        "        self.algo = \"PPO\"\n",
        "        self.num_gpus = 1\n",
        "        self.num_env_runners = 1\n",
        "        self.enable_new_api_stack = True\n",
        "        self.min_league_size = 1\n",
        "        self.win_rate_threshold = 0.95\n",
        "        self.num_episodes_human_play = 10\n",
        "        self.from_checkpoint = None\n",
        "        self.stop_iters = 100\n",
        "        self.stop_timesteps = 1000000\n",
        "        self.as_release_test = False\n",
        "        self.num_cpus = 1\n",
        "        self.local_mode = False\n",
        "        self.evaluation_interval = 10\n",
        "        self.evaluation_num_episodes = 10\n",
        "        self.evaluation_num_env_runners = 1\n",
        "        self.evaluation_duration = 10\n",
        "        self.evaluation_duration_unit = 10\n",
        "        self.evaluation_parallel_to_training = False\n",
        "        self.evaluation_config = {}\n",
        "        self.evaluation_num_workers = 1\n",
        "        self.log_level = \"INFO\"\n",
        "        self.output = \"ray_results\"\n",
        "        self.no_tune = False\n",
        "        self.num_agents = 2\n",
        "        self.num_iterations = 100\n",
        "        self.verbose = False\n",
        "        self.seed = 42\n",
        "        self.checkpoint_freq = 10\n",
        "        self.checkpoint_at_end = True\n",
        "        self.restore = None\n",
        "        self.num_samples = 1\n",
        "        self.max_concurrent_trials = 1\n",
        "\n",
        "config_args = Config()"
      ],
      "metadata": {
        "id": "CAdfXqWtjRHO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = add_rllib_example_script_args(default_timesteps=2000000)\n",
        "# parser.set_defaults(env=\"connect_four\")\n",
        "# parser.add_argument(\n",
        "#     \"--win-rate-threshold\",\n",
        "#     type=float,\n",
        "#     default=0.95,\n",
        "#     help=\"Win-rate at which we setup another opponent by freezing the \"\n",
        "#     \"current main policy and playing against a uniform distribution \"\n",
        "#     \"of previously frozen 'main's from here on.\",\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--min-league-size\",\n",
        "#     type=float,\n",
        "#     default=3,\n",
        "#     help=\"Minimum number of policies/RLModules to consider the test passed. \"\n",
        "#     \"The initial league size is 2: `main` and `random`. \"\n",
        "#     \"`--min-league-size=3` thus means that one new policy/RLModule has been \"\n",
        "#     \"added so far (b/c the `main` one has reached the `--win-rate-threshold \"\n",
        "#     \"against the `random` Policy/RLModule).\",\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--num-episodes-human-play\",\n",
        "#     type=int,\n",
        "#     default=10,\n",
        "#     help=\"How many episodes to play against the user on the command \"\n",
        "#     \"line after training has finished.\",\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--from-checkpoint\",\n",
        "#     type=str,\n",
        "#     default=None,\n",
        "#     help=\"Full path to a checkpoint file for restoring a previously saved \"\n",
        "#     \"Algorithm state.\",\n",
        "# )\n",
        "\n",
        "# args = parser.parse_args()"
      ],
      "metadata": {
        "id": "UADMYATei81H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "register_env(\"open_spiel_env\", lambda _: OpenSpielEnv(pyspiel.load_game(config_args.env)))\n",
        "\n",
        "def agent_to_module_mapping_fn(agent_id, episode, **kwargs):\n",
        "  # agent_id = [0|1] -> module depends on episode ID\n",
        "  # This way, we make sure that both modules sometimes play agent0\n",
        "  # (start player) and sometimes agent1 (player to move 2nd).\n",
        "  return \"main\" if hash(episode.id_) % 2 == agent_id else \"random\"\n",
        "\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "  return \"main\" if episode.episode_id % 2 == agent_id else \"random\"\n",
        "\n",
        "config = (\n",
        "get_trainable_cls(config_args.algo)\n",
        ".get_default_config()\n",
        ".api_stack(\n",
        "    enable_rl_module_and_learner=config_args.enable_new_api_stack,\n",
        "    enable_env_runner_and_connector_v2=config_args.enable_new_api_stack,\n",
        ")\n",
        ".environment(\"open_spiel_env\")\n",
        ".framework(config_args.framework)\n",
        "# Set up the main piece in this experiment: The league-bases self-play\n",
        "# callback, which controls adding new policies/Modules to the league and\n",
        "# properly matching the different policies in the league with each other.\n",
        ".callbacks(\n",
        "    functools.partial(\n",
        "        (\n",
        "            SelfPlayCallback\n",
        "            if config_args.enable_new_api_stack\n",
        "            else SelfPlayCallbackOldAPIStack\n",
        "        ),\n",
        "        win_rate_threshold=config_args.win_rate_threshold,\n",
        "    )\n",
        ")\n",
        ".env_runners(\n",
        "    num_env_runners=(config_args.num_env_runners or 2),\n",
        "    num_envs_per_env_runner=1 if config_args.enable_new_api_stack else 5,\n",
        ")\n",
        ".learners(\n",
        "    num_learners=config_args.num_gpus,\n",
        "    num_gpus_per_learner=1 if config_args.num_gpus else 0,\n",
        ")\n",
        ".resources(\n",
        "    num_cpus_for_main_process=1,\n",
        ")\n",
        ".multi_agent(\n",
        "    # Initial policy map: Random and default algo one. This will be expanded\n",
        "    # to more policy snapshots taken from \"main\" against which \"main\"\n",
        "    # will then play (instead of \"random\"). This is done in the\n",
        "    # custom callback defined above (`SelfPlayCallback`).\n",
        "    policies=(\n",
        "        {\n",
        "            # Our main policy, we'd like to optimize.\n",
        "            \"main\": PolicySpec(),\n",
        "            # An initial random opponent to play against.\n",
        "            \"random\": PolicySpec(policy_class=RandomPolicy),\n",
        "        }\n",
        "        if not config_args.enable_new_api_stack\n",
        "        else {\"main\", \"random\"}\n",
        "    ),\n",
        "    # Assign agent 0 and 1 randomly to the \"main\" policy or\n",
        "    # to the opponent (\"random\" at first). Make sure (via episode_id)\n",
        "    # that \"main\" always plays against \"random\" (and not against\n",
        "    # another \"main\").\n",
        "    policy_mapping_fn=(\n",
        "        agent_to_module_mapping_fn\n",
        "        if config_args.enable_new_api_stack\n",
        "        else policy_mapping_fn\n",
        "    ),\n",
        "    # Always just train the \"main\" policy.\n",
        "    policies_to_train=[\"main\"],\n",
        ")\n",
        ".rl_module(\n",
        "    model_config=DefaultModelConfig(fcnet_hiddens=[512, 512]),\n",
        "    rl_module_spec=MultiRLModuleSpec(\n",
        "        rl_module_specs={\n",
        "            \"main\": RLModuleSpec(),\n",
        "            \"random\": RLModuleSpec(module_class=RandomRLModule),\n",
        "        }\n",
        "    ),\n",
        ")\n",
        ")\n",
        "\n",
        "# Only for PPO, change the `num_epochs` setting.\n",
        "if config_args.algo == \"PPO\":\n",
        "  config.training(num_epochs=20)\n",
        "\n",
        "stop = {\n",
        "  NUM_ENV_STEPS_SAMPLED_LIFETIME: config_args.stop_timesteps,\n",
        "  TRAINING_ITERATION: config_args.stop_iters,\n",
        "  \"league_size\": config_args.min_league_size,\n",
        "}\n",
        "\n",
        "# Train the \"main\" policy to play really well using self-play.\n",
        "results = None\n",
        "if not config_args.from_checkpoint:\n",
        "    results = run_rllib_example_script_experiment(config, config_args, stop=stop)\n",
        "\n",
        "# Restore trained Algorithm (set to non-explore behavior) and play against\n",
        "# human on command line.\n",
        "if config_args.num_episodes_human_play > 0:\n",
        "    num_episodes = 0\n",
        "    config.explore = False\n",
        "    algo = config.build()\n",
        "    if config_args.from_checkpoint:\n",
        "        algo.restore(config_args.from_checkpoint)\n",
        "    else:\n",
        "        checkpoint = results.get_best_result().checkpoint\n",
        "        if not checkpoint:\n",
        "            raise ValueError(\"No last checkpoint found in results!\")\n",
        "        algo.restore(checkpoint)\n",
        "\n",
        "    # Play from the command line against the trained agent\n",
        "    # in an actual (non-RLlib-wrapped) open-spiel env.\n",
        "    human_player = 1\n",
        "    env = Environment(config_args.env)\n",
        "\n",
        "    while num_episodes < config_args.num_episodes_human_play:\n",
        "        print(\"You play as {}\".format(\"o\" if human_player else \"x\"))\n",
        "        time_step = env.reset()\n",
        "        while not time_step.last():\n",
        "            player_id = time_step.observations[\"current_player\"]\n",
        "            if player_id == human_player:\n",
        "                action = ask_user_for_action(time_step)\n",
        "            else:\n",
        "                obs = np.array(time_step.observations[\"info_state\"][player_id])\n",
        "                action = algo.compute_single_action(obs, policy_id=\"main\")\n",
        "                # In case computer chooses an invalid action, pick a\n",
        "                # random one.\n",
        "                legal = time_step.observations[\"legal_actions\"][player_id]\n",
        "                if action not in legal:\n",
        "                    action = np.random.choice(legal)\n",
        "            time_step = env.step([action])\n",
        "            print(f\"\\n{env.get_state}\")\n",
        "\n",
        "        print(f\"\\n{env.get_state}\")\n",
        "\n",
        "        print(\"End of game!\")\n",
        "        if time_step.rewards[human_player] > 0:\n",
        "            print(\"You win\")\n",
        "        elif time_step.rewards[human_player] < 0:\n",
        "            print(\"You lose\")\n",
        "        else:\n",
        "            print(\"Draw\")\n",
        "        # Switch order of players.\n",
        "        human_player = 1 - human_player\n",
        "\n",
        "        num_episodes += 1\n",
        "\n",
        "    algo.stop()"
      ],
      "metadata": {
        "id": "A5y6PBpEmTCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yaq0oAz0ml58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}