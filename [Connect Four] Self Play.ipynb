{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjy53NRRZfvqLnW3rYWcax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-connect-four/blob/main/%5BConnect%20Four%5D%20Self%20Play.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playing Connect Four using Self Play"
      ],
      "metadata": {
        "id": "9ddHPfUHmowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] gputil open_spiel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mfyOqFelr3N",
        "outputId": "73bb629e-3bb2-4c96-b495-a64de3060033"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: open_spiel in /usr/local/lib/python3.10/dist-packages (1.5)\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/dist-packages (2.38.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.2.2)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (16.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2024.6.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.28.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.3.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.13.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.12.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.9.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (24.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (24.2.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.4.0)\n",
            "Requirement already satisfied: ml-collections>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (21.6.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.18.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->ray[rllib]) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "from ray.air.constants import TRAINING_ITERATION\n",
        "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
        "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
        "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
        "from ray.rllib.env.utils import try_import_pyspiel, try_import_open_spiel\n",
        "from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv\n",
        "from ray.rllib.examples.rl_modules.classes.random_rlm import RandomRLModule\n",
        "from ray.rllib.examples.multi_agent.utils import (\n",
        "    ask_user_for_action,\n",
        "    SelfPlayCallback,\n",
        "    SelfPlayCallbackOldAPIStack,\n",
        ")\n",
        "from ray.rllib.examples._old_api_stack.policy.random_policy import RandomPolicy\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "from ray.rllib.utils.metrics import NUM_ENV_STEPS_SAMPLED_LIFETIME\n",
        "from ray.rllib.utils.test_utils import (\n",
        "    add_rllib_example_script_args,\n",
        "    run_rllib_example_script_experiment,\n",
        ")\n",
        "from ray.tune.registry import get_trainable_cls, register_env\n",
        "import platform\n",
        "\n",
        "import torch\n",
        "from importlib.metadata import version\n",
        "\n",
        "# Import after try_import_open_spiel, so we can error out with hints.\n",
        "from open_spiel.python.rl_environment import Environment  # noqa: E402"
      ],
      "metadata": {
        "id": "lJUAoPZeiGag"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('open_spiel')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('ray')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvbn9aJGh80w",
        "outputId": "fc5a17e6-cafa-469e-eec9-d7a2d2eda972"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.5.0+cu121\n",
            "Is Cuda Available: False\n",
            "Cuda Version: 12.1\n",
            "Numpy Version: 1.26.4\n",
            "Stable Baselines3 Version: 1.5\n",
            "Stable Baselines3 Version: 2.38.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open_spiel = try_import_open_spiel(error=True)\n",
        "pyspiel = try_import_pyspiel(error=True)"
      ],
      "metadata": {
        "id": "HkHVtp4zi_fQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "        self.env = \"connect_four\"\n",
        "        self.framework = \"torch\"\n",
        "        self.algo = \"PPO\"\n",
        "        self.num_gpus = 0\n",
        "        self.num_env_runners = 2\n",
        "        self.enable_new_api_stack = True\n",
        "        self.min_league_size = 3\n",
        "        self.win_rate_threshold = 0.95\n",
        "        self.num_episodes_human_play = 10\n",
        "        self.from_checkpoint = None\n",
        "        self.stop_iters = 100\n",
        "        self.stop_timesteps = 1000000\n",
        "        self.as_release_test = False\n",
        "        self.num_cpus = 1\n",
        "        self.local_mode = False\n",
        "        self.evaluation_interval = 10\n",
        "        self.evaluation_num_episodes = 10\n",
        "        self.evaluation_num_env_runners = 1\n",
        "        self.evaluation_duration = 10\n",
        "        self.evaluation_duration_unit = 10\n",
        "        self.evaluation_parallel_to_training = False\n",
        "        self.evaluation_config = {}\n",
        "        self.evaluation_num_workers = 1\n",
        "        self.log_level = \"INFO\"\n",
        "        self.output = \"ray_results\"\n",
        "        self.no_tune = False\n",
        "\n",
        "config_args = Config()"
      ],
      "metadata": {
        "id": "CAdfXqWtjRHO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = add_rllib_example_script_args(default_timesteps=2000000)\n",
        "# parser.set_defaults(env=\"connect_four\")\n",
        "# parser.add_argument(\n",
        "#     \"--win-rate-threshold\",\n",
        "#     type=float,\n",
        "#     default=0.95,\n",
        "#     help=\"Win-rate at which we setup another opponent by freezing the \"\n",
        "#     \"current main policy and playing against a uniform distribution \"\n",
        "#     \"of previously frozen 'main's from here on.\",\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--min-league-size\",\n",
        "#     type=float,\n",
        "#     default=3,\n",
        "#     help=\"Minimum number of policies/RLModules to consider the test passed. \"\n",
        "#     \"The initial league size is 2: `main` and `random`. \"\n",
        "#     \"`--min-league-size=3` thus means that one new policy/RLModule has been \"\n",
        "#     \"added so far (b/c the `main` one has reached the `--win-rate-threshold \"\n",
        "#     \"against the `random` Policy/RLModule).\",\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--num-episodes-human-play\",\n",
        "#     type=int,\n",
        "#     default=10,\n",
        "#     help=\"How many episodes to play against the user on the command \"\n",
        "#     \"line after training has finished.\",\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--from-checkpoint\",\n",
        "#     type=str,\n",
        "#     default=None,\n",
        "#     help=\"Full path to a checkpoint file for restoring a previously saved \"\n",
        "#     \"Algorithm state.\",\n",
        "# )\n",
        "\n",
        "# args = parser.parse_args()"
      ],
      "metadata": {
        "id": "UADMYATei81H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "register_env(\"open_spiel_env\", lambda _: OpenSpielEnv(pyspiel.load_game(config_args.env)))\n",
        "\n",
        "def agent_to_module_mapping_fn(agent_id, episode, **kwargs):\n",
        "  # agent_id = [0|1] -> module depends on episode ID\n",
        "  # This way, we make sure that both modules sometimes play agent0\n",
        "  # (start player) and sometimes agent1 (player to move 2nd).\n",
        "  return \"main\" if hash(episode.id_) % 2 == agent_id else \"random\"\n",
        "\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "  return \"main\" if episode.episode_id % 2 == agent_id else \"random\"\n",
        "\n",
        "config = (\n",
        "get_trainable_cls(config_args.algo)\n",
        ".get_default_config()\n",
        ".api_stack(\n",
        "    enable_rl_module_and_learner=config_args.enable_new_api_stack,\n",
        "    enable_env_runner_and_connector_v2=config_args.enable_new_api_stack,\n",
        ")\n",
        ".environment(\"open_spiel_env\")\n",
        ".framework(config_args.framework)\n",
        "# Set up the main piece in this experiment: The league-bases self-play\n",
        "# callback, which controls adding new policies/Modules to the league and\n",
        "# properly matching the different policies in the league with each other.\n",
        ".callbacks(\n",
        "    functools.partial(\n",
        "        (\n",
        "            SelfPlayCallback\n",
        "            if config_args.enable_new_api_stack\n",
        "            else SelfPlayCallbackOldAPIStack\n",
        "        ),\n",
        "        win_rate_threshold=config_args.win_rate_threshold,\n",
        "    )\n",
        ")\n",
        ".env_runners(\n",
        "    num_env_runners=(config_args.num_env_runners or 2),\n",
        "    num_envs_per_env_runner=1 if config_args.enable_new_api_stack else 5,\n",
        ")\n",
        ".learners(\n",
        "    num_learners=config_args.num_gpus,\n",
        "    num_gpus_per_learner=1 if config_args.num_gpus else 0,\n",
        ")\n",
        ".resources(\n",
        "    num_cpus_for_main_process=1,\n",
        ")\n",
        ".multi_agent(\n",
        "    # Initial policy map: Random and default algo one. This will be expanded\n",
        "    # to more policy snapshots taken from \"main\" against which \"main\"\n",
        "    # will then play (instead of \"random\"). This is done in the\n",
        "    # custom callback defined above (`SelfPlayCallback`).\n",
        "    policies=(\n",
        "        {\n",
        "            # Our main policy, we'd like to optimize.\n",
        "            \"main\": PolicySpec(),\n",
        "            # An initial random opponent to play against.\n",
        "            \"random\": PolicySpec(policy_class=RandomPolicy),\n",
        "        }\n",
        "        if not config_args.enable_new_api_stack\n",
        "        else {\"main\", \"random\"}\n",
        "    ),\n",
        "    # Assign agent 0 and 1 randomly to the \"main\" policy or\n",
        "    # to the opponent (\"random\" at first). Make sure (via episode_id)\n",
        "    # that \"main\" always plays against \"random\" (and not against\n",
        "    # another \"main\").\n",
        "    policy_mapping_fn=(\n",
        "        agent_to_module_mapping_fn\n",
        "        if config_args.enable_new_api_stack\n",
        "        else policy_mapping_fn\n",
        "    ),\n",
        "    # Always just train the \"main\" policy.\n",
        "    policies_to_train=[\"main\"],\n",
        ")\n",
        ".rl_module(\n",
        "    model_config=DefaultModelConfig(fcnet_hiddens=[512, 512]),\n",
        "    rl_module_spec=MultiRLModuleSpec(\n",
        "        rl_module_specs={\n",
        "            \"main\": RLModuleSpec(),\n",
        "            \"random\": RLModuleSpec(module_class=RandomRLModule),\n",
        "        }\n",
        "    ),\n",
        ")\n",
        ")\n",
        "\n",
        "# Only for PPO, change the `num_epochs` setting.\n",
        "if config_args.algo == \"PPO\":\n",
        "  config.training(num_epochs=20)\n",
        "\n",
        "stop = {\n",
        "  NUM_ENV_STEPS_SAMPLED_LIFETIME: config_args.stop_timesteps,\n",
        "  TRAINING_ITERATION: config_args.stop_iters,\n",
        "  \"league_size\": config_args.min_league_size,\n",
        "}\n",
        "\n",
        "# Train the \"main\" policy to play really well using self-play.\n",
        "results = None\n",
        "if not config_args.from_checkpoint:\n",
        "    results = run_rllib_example_script_experiment(config, config_args, stop=stop)\n",
        "\n",
        "# Restore trained Algorithm (set to non-explore behavior) and play against\n",
        "# human on command line.\n",
        "if config_args.num_episodes_human_play > 0:\n",
        "    num_episodes = 0\n",
        "    config.explore = False\n",
        "    algo = config.build()\n",
        "    if config_args.from_checkpoint:\n",
        "        algo.restore(config_args.from_checkpoint)\n",
        "    else:\n",
        "        checkpoint = results.get_best_result().checkpoint\n",
        "        if not checkpoint:\n",
        "            raise ValueError(\"No last checkpoint found in results!\")\n",
        "        algo.restore(checkpoint)\n",
        "\n",
        "    # Play from the command line against the trained agent\n",
        "    # in an actual (non-RLlib-wrapped) open-spiel env.\n",
        "    human_player = 1\n",
        "    env = Environment(config_args.env)\n",
        "\n",
        "    while num_episodes < config_args.num_episodes_human_play:\n",
        "        print(\"You play as {}\".format(\"o\" if human_player else \"x\"))\n",
        "        time_step = env.reset()\n",
        "        while not time_step.last():\n",
        "            player_id = time_step.observations[\"current_player\"]\n",
        "            if player_id == human_player:\n",
        "                action = ask_user_for_action(time_step)\n",
        "            else:\n",
        "                obs = np.array(time_step.observations[\"info_state\"][player_id])\n",
        "                action = algo.compute_single_action(obs, policy_id=\"main\")\n",
        "                # In case computer chooses an invalid action, pick a\n",
        "                # random one.\n",
        "                legal = time_step.observations[\"legal_actions\"][player_id]\n",
        "                if action not in legal:\n",
        "                    action = np.random.choice(legal)\n",
        "            time_step = env.step([action])\n",
        "            print(f\"\\n{env.get_state}\")\n",
        "\n",
        "        print(f\"\\n{env.get_state}\")\n",
        "\n",
        "        print(\"End of game!\")\n",
        "        if time_step.rewards[human_player] > 0:\n",
        "            print(\"You win\")\n",
        "        elif time_step.rewards[human_player] < 0:\n",
        "            print(\"You lose\")\n",
        "        else:\n",
        "            print(\"Draw\")\n",
        "        # Switch order of players.\n",
        "        human_player = 1 - human_player\n",
        "\n",
        "        num_episodes += 1\n",
        "\n",
        "    algo.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "A5y6PBpEmTCW",
        "outputId": "35aefc7e-26a9-41bb-a727-c61dfdf302f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "2024-10-25 02:59:54,737\tINFO worker.py:1816 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Config' object has no attribute 'no_tune'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8fb65c9b6d62>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_rllib_example_script_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Restore trained Algorithm (set to non-explore behavior) and play against\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/test_utils.py\u001b[0m in \u001b[0;36mrun_rllib_example_script_experiment\u001b[0;34m(base_config, args, stop, success_metric, trainable, tune_callbacks, keep_config, scheduler, progress_reporter)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m     \u001b[0;31m# Run the experiment w/o Tune (directly operate on the RLlib Algorithm object).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_tune\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_test\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_release_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'no_tune'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-D-r7UM_okk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yaq0oAz0ml58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}